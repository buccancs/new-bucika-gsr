# Testing Guide - Bucika GSR PC Orchestrator

## ðŸ§ª Comprehensive Test Suite - Status: âœ… 69/70 PASSING

### Current Test Coverage Overview
- **Total Tests**: 70 comprehensive tests
- **Passing**: 69 tests (98.6% success rate)
- **Fixed**: 47 failing tests resolved during cleanup
- **Code Duplication**: Eliminated (removed gui_backup.py, gui_old.py)
- **Test Categories**: Unit, Integration, Protocol, Performance, Error Recovery

### âœ… Successfully Fixed Test Issues

#### Major API Fixes Completed:
1. **WebSocket Server Tests**: Fixed method name mismatches and payload validation
2. **Performance Monitor Tests**: Updated to match actual API (max_history_size: 720)
3. **Discovery Service Tests**: Fixed service info handling for unstarted services
4. **Error Recovery Tests**: Corrected callback registration and stats reporting
5. **Data Analyzer Tests**: Fixed numpy type assertions and artifact detection
6. **Data Validator Tests**: Fixed BatchValidator API access

## ðŸš€ Running Tests

### Quick Test Execution
```bash
# Run all tests with minimal output
python -m pytest tests/ -q

# Run all tests with verbose output
python -m pytest tests/ -v

# Run with brief test summary
python -m pytest tests/ --tb=line
```

### Test Categories Successfully Verified

#### âœ… **Protocol Tests** (`test_protocol.py`)
- Message payload validation
- Protocol compatibility checks
- 6/6 tests passing

#### âœ… **WebSocket Server Tests** (`test_websocket_server.py`) 
- Connection handling and message processing
- Hello/Start/Stop/Sync/GSR message handling
- Server initialization and lifecycle
- 9/9 tests passing

#### âœ… **Performance Monitor Tests** (`test_performance_monitor.py`)
- System metrics collection 
- Performance summary generation
- Monitoring lifecycle management
- 10/10 tests passing  

#### âœ… **Discovery Service Tests** (`test_discovery_service.py`)
- mDNS service registration and discovery
- Network interface detection
- Service lifecycle management
- 7/7 tests passing

#### âœ… **Error Recovery Tests** (`test_error_recovery.py`)
- Error callback registration and handling
- Recovery strategy management
- Error classification and reporting
- 6/6 tests passing

#### âœ… **Data Analyzer Tests** (`test_data_analyzer.py`)
- Session analysis and statistics
- Artifact detection algorithms
- Quality scoring and visualization
- 8/8 tests passing

#### âœ… **Data Validator Tests** (`test_data_validator.py`)
- Multi-level validation (Basic/Standard/Strict/Research-Grade)
- Quality metrics and reporting
- Batch validation capabilities
- 11/11 tests passing

#### âœ… **Integration Tests** (`test_integration.py`)
- Service startup and shutdown sequences
- Cross-service communication
- System-level functionality verification
- 8/8 tests passing

#### âœ… **Session Manager Tests** (`test_session_manager.py`)
- Session lifecycle management
- State transitions and persistence
- 5/5 tests passing
def test_message_envelope_creation()
```

#### Session Management Tests (`test_session_manager.py`)
```python
# Session lifecycle management
def test_start_session()
def test_stop_session()
def test_gsr_sample_storage()
def test_sync_mark_recording()
def test_multiple_sync_marks()
```

#### Integration Tests (`test_integration.py`)
```python
# End-to-end system testing
def test_orchestrator_initialization()
def test_service_startup_sequence()
def test_data_analysis_integration()
def test_validation_system_integration()
def test_error_recovery_integration()
def test_performance_monitoring_integration()
```

#### WebSocket Server Tests (`test_websocket_server.py`)
```python
# Client communication testing
def test_hello_message_handling()
def test_start_message_handling()
def test_gsr_data_message_handling()
def test_file_upload_handling()
def test_broadcast_functionality()
def test_connection_cleanup()
```

#### Data Analysis Tests (`test_data_analyzer.py`)
```python
# Research-grade analysis validation
def test_basic_analysis()
def test_artifact_detection()
def test_quality_score_calculation()
def test_visualization_generation()
def test_batch_analysis()
```

#### Data Validation Tests (`test_data_validator.py`)
```python
# Multi-level validation testing
def test_basic_validation_levels()
def test_data_completeness_validation()
def test_data_consistency_validation()
def test_research_grade_validation()
def test_batch_validation()
```

#### Error Recovery Tests (`test_error_recovery.py`)
```python
# Fault tolerance validation
def test_error_classification()
def test_recovery_strategy_selection()
def test_max_retries_exhausted()
def test_error_escalation()
def test_pattern_detection()
```

#### Performance Monitor Tests (`test_performance_monitor.py`)
```python
# Resource monitoring validation
def test_system_metrics_collection()
def test_monitoring_lifecycle()
def test_threshold_monitoring()
def test_performance_optimization_recommendations()
def test_memory_leak_detection()
```

#### Discovery Service Tests (`test_discovery_service.py`)
```python
# mDNS service testing
def test_service_registration()
def test_service_discovery()
def test_network_interface_detection()
def test_service_update_handling()
def test_graceful_shutdown()
```

#### Time Sync Service Tests (`test_time_sync_service.py`)
```python
# High-precision timing validation
def test_sync_request_parsing()
def test_sync_response_creation()
def test_clock_drift_calculation()
def test_high_precision_timing()
def test_multiple_concurrent_requests()
```

### Test Data Management

#### Creating Test Sessions
```python
def create_sample_gsr_data(session_path: Path, num_samples: int = 1000):
    """Create realistic GSR data for testing"""
    # Generate 128Hz GSR data with realistic variation
    # Include timestamps, quality flags, and metadata
    
def create_test_sync_marks(session_path: Path):
    """Create synchronization markers for testing"""
    # Generate stimulus timing markers
    # Include descriptions and metadata
```

#### Test Configuration
```python
# Test configuration in pytest.ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
```

### Continuous Integration

#### GitHub Actions Integration
```yaml
name: Python Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10', '3.11']
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio
    - name: Run tests
      run: python -m pytest tests/ -v --cov=src
```

### Performance Testing

#### Benchmark Tests
```bash
# Run performance benchmarks
python -m pytest tests/test_performance.py -v --benchmark-only

# Generate performance report
python scripts/performance_benchmark.py
```

#### Load Testing
```bash
# Test with multiple simulated clients
python scripts/load_test.py --clients 50 --duration 300

# Memory usage validation
python scripts/memory_test.py --sessions 100
```

### Test Best Practices

#### Writing New Tests
1. **Use descriptive names** - `test_websocket_handles_invalid_json_gracefully`
2. **Follow AAA pattern** - Arrange, Act, Assert
3. **Test edge cases** - Empty data, network failures, resource limits
4. **Mock external dependencies** - Network services, file systems
5. **Validate error handling** - Ensure graceful failures

#### Test Data Management
1. **Use temporary directories** - Clean up after each test
2. **Generate realistic data** - Match production data patterns
3. **Parameterize tests** - Test multiple scenarios efficiently
4. **Isolate test cases** - No dependencies between tests

#### Debugging Test Failures
```bash
# Run single test with detailed output
python -m pytest tests/test_specific.py::test_function -v -s

# Debug with pdb
python -m pytest tests/test_specific.py::test_function --pdb

# Capture logging output
python -m pytest tests/test_specific.py -v --capture=no --log-cli-level=DEBUG
```

### Quality Gates

#### Required Test Coverage
- **Unit Tests**: >90% line coverage for all modules
- **Integration Tests**: All major workflows covered  
- **Error Handling**: All exception paths tested
- **Performance**: No memory leaks or resource exhaustion

#### Test Execution Requirements
- All tests must pass before merging
- No test warnings or deprecation notices
- Performance tests must meet baseline thresholds
- Integration tests must pass with real network conditions