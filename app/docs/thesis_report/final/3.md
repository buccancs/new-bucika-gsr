# Chapter 3: Requirements

## 3.1 Problem Statement and Research Context

The system is developed to support contactless Galvanic Skin Response
(GSR) prediction research. Traditional GSR measurement requires
contact sensors attached to a person's skin, but this project aims to
bridge contact-based and contact-free physiological monitoring. The
system allows researchers to collect synchronised multi-modal data
by combining wearable GSR readings with contactless signals such
as thermal imagery and video. This data supports developing models to
predict GSR without direct skin contact. This addresses a key research
gap by providing a reliable way to acquire ground-truth GSR data
alongside contactless sensor inputs, with all data streams synchronised
for analysis [1].

In the context of physiological and affective computing research, the
focus is on stress and emotion analysis, where GSR is a common
measure of sympathetic nervous system activity. The system integrates
thermal cameras, RGB video, and inertial sensors with GSR to build a
rich dataset for exploring how observable signals (such as facial
thermal patterns or motion) correlate with changes in skin conductance.
This multi-sensor platform operates in real-world settings (e.g.
labs or field studies) and emphasises temporal precision and data
integrity, so that subtle physiological responses are captured and can
be later aligned for machine learning [1]. Overall, the system's goal
is to enable experiments that simultaneously record a participant's
physiological responses alongside visual and thermal cues, laying a
foundation for contactless stress detection.

## 3.2 Requirements Engineering Approach

The system's requirements were derived with an iterative,
research-driven approach [12]. Initially, high-level objectives (e.g.
"enable synchronised GSR and video recording") were identified from
the research goals. These were then refined through requirements
elicitation involving the researchers' needs and hardware constraints.
The project adopted a prototyping methodology: early versions of the
system were built and tested, and feedback was used to update the
requirements. For example, during development, additional needs like
data encryption and device fault tolerance emerged and were
added to the requirements (as evident from commit history introducing
security checks and recovery features).

Requirements engineering was performed in alignment with IEEE guidelines
[11]. Each requirement was documented with a unique ID and categorised
(functional vs. non-functional). The implementation closely tracked the
requirements -- the repository structure and commit messages show that
whenever a new capability was implemented (e.g. a calibration module
or time synchronisation service), it corresponded to a defined
requirement. Traceability was maintained through the comprehensive
matrix presented in Section 3.6. Overall, the approach was
incremental and user-focused: starting from the core research use
cases and continuously refining the system requirements as technical
insights were gained during development.

## 3.3 Functional Requirements Overview

The system's functional requirements are listed below (FR#). Each
requirement is stated in terms of what the system shall do. These
requirements were derived from the system's implemented capabilities and
verified via the source code:

- FR1: Multi-Device Sensor Integration -- The system shall support
  connecting and managing multiple sensor devices simultaneously. This
  includes discovering and pairing Shimmer GSR sensors via direct
  Bluetooth or through an Android device acting as a bridge. If no real
  sensors are available, the system shall offer a simulation mode to
  generate dummy sensor data for testing (see Appendix F.3 for implementation details).

- FR2: Synchronised Multi-Modal Recording -- The system shall start
  and stop data recording synchronously across all connected
  devices. When a session starts, the PC instructs each Android device
  to begin recording GSR data, video (RGB camera), and thermal
  imaging in parallel. At the same time, the PC begins logging
  data from any directly connected Shimmer sensors. All streams share a
  common session timestamp to enable later alignment.

- FR3: Time Synchronisation Service -- The system shall synchronise
  clocks across devices to ensure all data is time-aligned. The PC runs
  a time synchronisation service (e.g. an NTP-like server on the local
  network) so that each Android device can calibrate its clock to the
  PC's clock before and during recording. This achieves a
  sub-millisecond timestamp accuracy between GSR readings and video
  frames, which is crucial for data integrity.

- FR4: Session Management -- The system shall organise recordings
  into sessions, each with a unique ID or name. It shall allow the
  researcher to create a new session (automatically timestamped) and
  later terminate the session when finished. Upon session start, the
  PC creates a directory for data storage and initialises a session
  metadata file. When the session ends, the metadata (start/end
  time, duration, status) is finalised and saved. Only one session
  can be active at a time, preventing overlap.

- FR5: Data Recording and Storage -- For each session, the system
  shall record: (a) Physiological sensor data from the Shimmer GSR
  module (including GSR and any other channels such as PPG or
  accelerometer, sampled at 128 Hz), and (b) video and thermal
  data from each Android device (with at least 1920×1080 video at
  30 FPS). Sensor readings stream to the PC in real time and are
  written to local CSV files as they arrive to avoid data loss. Each Android device stores its own raw video/thermal files locally
  during recording and transfers them to the PC after the session (see
  FR10). The system shall also handle audio recording if enabled (e.g.
  microphone audio at 44.1 kHz), keeping it synchronised with the other
  streams.

- FR6: User Interface for Monitoring & Control -- The system shall
  provide a GUI on the PC for the researcher to control sessions and
  monitor devices. This interface should list connected devices and
  their status (e.g. battery level, streaming/recording state),
  allow the user to start/stop sessions, and display indicators like
  recording timers and sample counts. The GUI should also show preview
  feeds or periodic status updates (for example, updating every few
  seconds with the number of samples received). If a device
  disconnects or encounters an error, the UI should highlight that
  device so the researcher can take appropriate action.

- FR7: Device Synchronisation and Signals -- The system shall
  coordinate multiple devices by sending control commands and
  synchronisation cues. For example, the PC can broadcast a
  synchronisation signal (such as a screen flash or buzzer) to all
  Android devices to mark the same moment across all video recordings.
  These signals (e.g. a visual flash on each device's screen)
  help with aligning footage during analysis. The system uses a
  JSON-based command protocol so that the PC can instruct all devices to
  start/stop recording and perform other actions in unison.

- FR8: Fault Tolerance and Recovery -- If a device (Android or
  sensor) disconnects or fails during a session, the system shall detect
  the issue and continue the session with the remaining devices.
  The PC will log a warning and mark that device as offline. When
  the device reconnects, it should seamlessly rejoin the ongoing
  session. The system will attempt to recover the device's session
  state by re-synchronising and executing any commands that were queued
  while it was offline. This ensures a temporary network drop
  does not invalidate the entire session.

- FR9: Calibration Utilities -- The system shall include tools for
  calibrating sensors and cameras. In particular, it provides a
  procedure to align the thermal camera's field of view with the RGB
  camera (e.g. using a checkerboard pattern). The researcher can perform
  a calibration session where images are captured and calibration
  parameters are computed. Calibration settings (such as pattern type,
  pattern size, and number of images) are adjustable in the system
  configuration. The resulting calibration data is saved so that
  recorded thermal and visual data can be accurately merged during
  analysis. (This requirement is derived from the presence of a
  calibration module in the code.)

- FR10: Data Transfer and Aggregation -- Once a session is stopped,
  the system shall transfer all recorded data from each Android device
  to the PC. The Android application packages the session's files (e.g.
  video, thermal images, local sensor logs) and sends them to the PC
  over the network. The PC saves each incoming file in the
  session folder and updates the session metadata with that file's entry
  (including file type and size). This automation ensures the
  researcher can retrieve all data without manually offloading devices.
  If any file fails to transfer, the system logs an error and (if
  possible) retries, so that data is not lost.

The above functional requirements are realised in various components
of the code. For instance, connecting multiple devices and recording
them simultaneously is handled by the ShimmerPCApplication.start_session
logic, and session metadata management is implemented in the SessionManager
class. Detailed implementation examples are provided in Appendix F.

## 3.4 Non-Functional Requirements

In addition to core functionality, the system must meet several
non-functional requirements to ensure it is suitable for a research
setting. These include:

- NFR1: Performance (Real-Time Data Handling) -- The system must
  handle data in real time, with minimal latency and sufficient
  throughput. It should support at least 128 Hz sensor sampling and
  30 FPS video recording concurrently without data loss or buffering
  issues. The design uses multi-threading and asynchronous processing to
  achieve this. Video is recorded at \~5 Mbps and audio at 128 kbps,
  which the system writes to storage in real time. Even with
  multiple devices (e.g. 3+ cameras and a GSR sensor), the system should
  not drop frames or samples due to performance bottlenecks.

- NFR2: Temporal Accuracy -- Clock synchronisation accuracy between
  devices should be on the order of milliseconds or better. The system's
  built-in NTP time server and sync protocol aim to keep timestamp
  differences very low (e.g. \<5 ms offset and jitter as logged
  after synchronisation). This is critical for valid sensor
  fusion; therefore, the system continuously synchronises all device
  clocks during a session. Timestamp precision is maintained in all logs
  (to the millisecond), and all devices use the PC's clock as the
  reference.

- NFR3: Reliability and Fault Tolerance -- The system must be
  robust to interruptions. If a sensor or network link fails, the rest
  of the system continues recording unaffected (as per FR8). Data
  already recorded should be safely preserved even if a session ends
  unexpectedly (e.g. the PC application crashes). The session design
  ensures that files are written incrementally and closed properly on
  stop, to avoid corruption. A recovery mechanism is in place to
  handle device reconnections (queuing messages while a device is
  offline). In addition, the Shimmer device interface includes an
  auto-reconnect feature to attempt re-establishing Bluetooth
  connections automatically.

- NFR4: Data Integrity and Validation -- All recorded data should
  be accurate and free from corruption. The system has a data validation
  mode for sensor data that checks incoming values are within
  expected ranges (for example, verifying GSR readings stay between 0.0
  and 100.0 μS). Each file transfer from devices is verified for
  completeness (expected file sizes are known and logged in metadata).
  Session metadata acts as a manifest so that missing or
  inconsistent files can be detected easily. Also, the system will not
  overwrite existing session data -- each session is stored in a unique
  timestamped folder to avoid conflicts.

- NFR5: Security -- The system must safeguard the security and
  privacy of the recorded data. All network communication between the PC
  and the Android devices is encrypted (TLS is enabled in the
  configuration). The system requires authentication tokens
  for device connections (with a configurable minimum length of 32
  characters) to prevent unauthorised devices from joining the session.
  Security checks at startup will warn if encryption or
  authentication is not properly configured. All recorded data
  files are stored locally on the PC; if cloud or external transfer is
  needed, it is performed only by the researcher (there is no
  inadvertent data upload). Additionally, the system checks file
  permissions and the runtime environment on startup to avoid insecure
  defaults.

- NFR6: Usability -- The system should be easy to use for
  researchers who are not software experts. The PC's graphical interface
  is designed to be intuitive, with clear controls to start/stop
  sessions and indicators for system status. For example, the UI shows a
  recording indicator when a session is active and displays device
  statuses (connected/disconnected, recording, battery level) in real
  time. Sensible default settings (e.g. a default dark theme and
  window size) ensure a good user experience out of the box. The
  Android app requires minimal user interaction after initial setup ---
  typically the researcher just needs to mount the devices and tap
  "Connect", with the PC orchestrating the rest. User manuals or
  on-screen guidance are provided for tasks like calibration.

- NFR7: Scalability -- The system's architecture should scale to
  accommodate multiple devices and long recording durations. It has been
  tested with up to *8* Android devices streaming or recording
  concurrently (the configuration allows up to 10 connections). Similarly, the system supports sessions up to at least *120 minutes*
  in duration by default. To manage large video files, recordings
  can be chunked into \~1 GB segments automatically so that individual
  file sizes remain manageable. This ensures that even
  high-resolution, extended sessions do not overwhelm the file system or
  hinder post-processing.

- NFR8: Maintainability and Modularity -- The system is built in a
  modular way to simplify maintenance. Components are separated (e.g.,
  Calibration Manager, Session Manager, Shimmer Manager,
  Network Server) and communicate via clear interfaces. This modular
  design (evident in the repository structure) makes it easier to update
  one part (such as swapping out the thermal camera SDK) without
  affecting others. Configuration is externalised (through `config.json`
  and other settings), so that changes in requirements (e.g.,
  adding new sensor types or changing sampling rates) can be
  accommodated by editing configuration rather than modifying code.
  Finally, the project includes test scripts and extensive logging to
  aid in debugging, which contributes to maintainability.

The non-functional requirements were validated through system testing
and by reviewing configuration parameters. For example, the presence of
TLS settings and runtime security checks shows that the security
requirements were met, and the multi-threaded design and resource limits
set in the config file indicate that performance requirements were
satisfied.

## 3.5 Use Case Scenarios

To illustrate how the system is intended to be used, this section
describes several use case scenarios. Each scenario outlines the
typical interaction between the user (researcher) and the system,
along with how the system's components work together to fulfil the
requirements.

### Use Case 1: Conducting a Multi-Modal Recording Session

Description: A researcher initiates and completes a recording
session capturing GSR data alongside video and thermal streams from
multiple devices. This is the primary use of the system, corresponding
to a live experiment with a participant.

- Primary Actor: Researcher (system operator).

- Secondary Actors: Participant (subject being recorded), though
  they do not directly interact with the system UI.

- Preconditions:

- The Shimmer GSR sensor is charged and either connected to the PC (via
  Bluetooth dongle) or paired with an Android device.

- Android recording devices are powered on, running the recording app,
  and on the same network as the PC. The PC application is running and
  all devices have synchronised their clocks (either via initial NTP
  sync or prior calibration).

- The researcher has configured any necessary settings (e.g. chosen a
  session name, verified camera focus, etc.).

- Main Flow:

- The Researcher opens the PC control interface and creates a new
  session (providing a session name or accepting a default). The
  system validates the name and creates a session folder and metadata
  file on disk. The session is now "active" but not recording yet.

- The Researcher selects the devices to use. For example, they ensure
  the Shimmer sensor appears in the device list and one or more Android
  devices show as "connected" in the UI. If the Shimmer is not yet
  connected, the Researcher clicks "Scan for Devices." The system
  performs a scan: it finds the Shimmer sensor either directly via
  Bluetooth or through an Android's paired devices. The Researcher
  then clicks "Connect" for the Shimmer. The system establishes a
  connection (or uses a simulated device if the real sensor is
  unavailable) and updates the UI status to "Connected."

- The Researcher checks that video previews from each Android (if
  available) are showing in the UI (small preview panels) and that the
  GSR signal is streaming (e.g. a live plot or at least a sample counter
  incrementing). Internally, the PC has started a background thread
  receiving data from the Shimmer sensor continuously. The system
  also maintains a heartbeat to each Android (pinging every few seconds)
  to ensure connectivity.

- The Researcher initiates recording by clicking "Start Recording." The
  PC sends a start command to all connected Android devices (with a
  session ID). Each Android begins recording its camera (and thermal
  sensor, if present) and optionally starts streaming its own sensor
  data (if any) back to PC. Simultaneously, the PC instructs the
  Shimmer Manager to start logging data to file. This is done
  nearly simultaneously for all devices. The PC's session manager marks
  the session status as "recording" and timestamps the start time.

- During the recording, the Researcher can observe real-time status. For
  example, the UI might display the elapsed time, the number of data
  samples received so far, and the count of connected devices. Every
  30 seconds, the system logs a status summary (e.g., "Status: 1
  Android, 1 Shimmer, 3000 samples"). If the Researcher has a
  specific event to mark, they can trigger a sync signal: for instance,
  pressing a "Flash Sync" button. When pressed, the system calls
  `send_sync_signal` to all Androids to flash their screen LEDs
  (creating a visible marker in the videos) and logs the event in the
  GSR data stream.

- If any device disconnects mid-session (e.g. an Android phone's WiFi
  drops out), the system warns the Researcher via the UI (perhaps
  highlighting that device in red). The recording on that device might
  continue offline (the Android app will still save its local video).
  The PC's Session Synchroniser marks the device as offline and queues
  any commands for it. The Researcher can continue the session if
  the other streams are still running. When the disconnected device
  comes back online (e.g. WiFi reconnects), the system automatically
  detects it, re-synchronises the session state, and executes any missed
  commands (all in the background). This recovery happens without
  user intervention, ensuring the session can proceed.

- The Researcher decides to end the recording after, say, 15 minutes.
  They click "Stop Recording" on the PC interface. The PC sends stop
  commands to all Androids, which then cease recording their cameras.
  The Shimmer Manager stops logging GSR data at the same time.
  Each component flushes and closes its output files. The session
  manager marks the session as completed, calculates the duration, and
  updates the session metadata (end time, duration, and status). A
  log message confirms the session has ended along with its duration and
  sample count stats.

- After stopping, the system automatically initiates data transfer
  from the Android devices. The File Transfer Manager on each Android
  packages the recorded files (e.g., `video_20250807_...mp4`, thermal
  data, etc.) and begins sending them to the PC, one file at a time.
  The PC receives each file (via its network server) and saves
  it into the session folder, simultaneously calling
  `SessionManager.add_file_to_session()` to record the file's name and
  size in the metadata. A progress indicator may be shown to the
  Researcher.

- Once all files are transferred, the system notifies the Researcher
  that the session data collection is complete (e.g., "Session
  15min_stress_test completed -- 5 files saved"). The Researcher can
  then optionally review summary statistics (the UI might show, for
  example, average GSR level, or simply confirm the number of files and
  total data size). The session is now closed and all resources are
  cleaned up.

- Postconditions: All recorded data (GSR CSV, video files, etc.) are
  safely stored in the PC's session directory. The session metadata JSON
  lists all devices that participated and all files collected. The
  system remains running, and the researcher could start a new session
  if needed. The participant's involvement is done, and the data are
  ready for analysis (outside the scope of the recording system). If any
  device failed to transfer data, the researcher is made aware so they
  can retrieve it manually if possible.

- Alternate Flows:\
  a. *No Shimmer available:* If the Shimmer sensor is not connected or
  malfunctions, the Researcher can still run a session with just
  video/thermal. The system will log that no GSR device is present, and
  it can operate in a video-only mode (possibly using a simulated GSR
  signal for demonstration).\
  b. *Calibration needed:* If this is the first session or devices have
  been re-arranged, the Researcher might perform a calibration
  routine before step 4. In that case, they would use the Calibration
  Utility (see Use Case 2) to calibrate cameras. Once calibration is
  done and saved, the recording session proceeds as normal.\
  c. *Device battery low:* During step 5, if an Android's battery is
  critically low, the system could alert the Researcher (since the
  device status includes battery level). The Researcher might
  decide to stop the session early or replace the device. The system
  will include the battery status in metadata for transparency.\
  d. *Network loss at end:* If the network connection to a device is
  lost exactly when "Stop" is pressed, the PC might not immediately
  receive confirmation from that device. In this case, the PC will mark
  the device as offline (as in step 6) and proceed to finalise the
  session with whatever data it has. Later, when the device reconnects,
  the Session Synchroniser can still trigger the file transfer for that
  device's data so it eventually gets saved on the PC.

### Use Case 2: Camera Calibration for Thermal Alignment

Description: Before conducting recordings that involve a thermal
camera, the researcher performs a calibration procedure to align the
thermal camera's view with the RGB camera view. This ensures that data
from these two modalities can be compared pixel-to-pixel in analysis.

- Primary Actor: Researcher.

- Preconditions: At least one Android device with both an RGB and a
  thermal camera (or an external thermal camera attached) is available.
  A calibration pattern (e.g. a black-and-white checkerboard) is printed
  and ready. The system's calibration settings (pattern size, etc.) are
  configured if needed.

- Main Flow:

- The Researcher opens the Calibration Tool in the PC application
  (or on the Android app, depending on implementation -- assume PC-side
  coordination). They select the device(s) to calibrate (e.g.,
  "Device A -- RGB + Thermal").

- The system instructs the device to enter calibration mode. Typically,
  the Android app might open a special calibration capture activity
  (with perhaps an overlay or just using both cameras). The Researcher
  holds the checkerboard pattern in front of the cameras and ensures it
  is visible to both the RGB and thermal cameras.

- The Researcher initiates capture (maybe pressing a "Capture Image"
  button). The device (or PC via the device) captures a pair of
  images -- one from the RGB camera and one from the thermal camera --
  at the same moment. It may need multiple images from different angles;
  the configuration might specify capturing, say, 10 images. The
  system gives feedback after each capture (e.g., "Image 1/10
  captured").

- After the required number of calibration images are collected, the
  Researcher clicks "Compute Calibration." The system runs a calibration
  algorithm (likely implementing Zhang's method for camera calibration)
  on the collected image pairs. This computes parameters like camera
  intrinsics for each camera and the extrinsic transform aligning
  thermal to RGB.

- The system stores the resulting calibration parameters (e.g. in a
  calibration result file or in config). It also might display an
  estimate of calibration error (so the Researcher can judge quality).
  For instance, if the reprojection error exceeds the threshold
  (say threshold = 1.0 pixel), the system might warn that the
  calibration quality is low.

- The Researcher is satisfied with the calibration (error is
  acceptable). They save the calibration profile. Now the system will
  use this calibration data in future sessions to correct or align
  thermal images to the RGB frame if needed (this might be done in
  post-processing rather than during recording). The Researcher exits
  the calibration mode.

- Alternate Flows:\
  a. *Calibration failure:* If the system cannot detect the calibration
  pattern in the images (e.g., poor contrast in thermal image), it
  notifies the Researcher. The Researcher can then recapture images
  (maybe adjust the pattern distance or lighting) until the system
  successfully computes a calibration.\
  b. *Partial calibration:* The Researcher may choose to only calibrate
  intrinsics of each camera separately (for example, if thermal-RGB
  alignment is less important than ensuring each camera's lens
  distortion is corrected). In this case, the flow would be adjusted to
  capturing images of a known grid for each camera independently.\
  c. *Using stored calibration:* If calibration was done previously, the
  Researcher might skip this use case entirely and rely on the stored
  calibration parameters. The system allows loading a saved calibration
  file, which then becomes active for subsequent recordings.

Use Case 3 (Secondary): Reviewing and Managing Session Data
(Optional) -- *This use case would describe how a researcher can use the
system to review past session metadata and possibly replay or export
data.* (For brevity, this is not expanded here, but the system does
include features like session listing and possibly data export tools,
given that a web UI template for sessions exists.)

*(The above scenarios demonstrate the system's functionality in context.
They confirm that the requirements -- from multi-device synchronisation
to calibration -- all serve real user workflows. The sequence of
interactions in Use Case 1, especially, shows how the system meets the
need for synchronised multi-modal data collection in a practical
experiment setting.)*

## 3.6 System Analysis (Architecture & Data Flow)

System Architecture: The system adopts a distributed
architecture with a central PC Controller and multiple Mobile
Recording Units. The PC (a Python desktop application) acts as the
master, coordinating all devices, while each Android device runs a
recording application that functions as a client node. This architecture
is essentially a hub-and-spoke topology, where the PC hub maintains
control and timing, and the spokes (sensors/cameras) carry out data
collection.

On the PC side, the software is organised into modular managers, each
responsible for a subset of functionality: -- The Session Manager
handles the overall session lifecycle (creation, metadata logging, and
closure). -- The Network Server component (within the
`AndroidDeviceManager` and `PCServer` classes) manages communication
with Android devices over TCP/IP (listening on a specified port, e.g.
9000). It uses a custom JSON-based protocol for commands and
status messages. -- The Shimmer Manager deals with the Shimmer GSR
sensors, including Bluetooth connectivity (via the PyShimmer library if
available) and data streaming to the PC. It also multiplexes
data from multiple sensors and writes sensor data to CSV files in
real-time. -- The Time synchronisation Service (Master Clock) runs
on the PC to keep device clocks aligned. As seen in the code, an
`NTPTimeServer` thread on the PC listens on a port (e.g. 8889) and
services time-sync requests from clients. It periodically syncs
with external NTP sources for accuracy and provides time offset
information to the Android devices, which adjust their local clocks
accordingly. -- The GUI Module (built with PyQt5 or a similar
framework) provides the desktop interface. It includes panels for device
status, session control, and live previews. This GUI updates based on
callbacks and status data from the managers (for instance, when a new
device connects, the Shimmer Manager invokes a callback that the GUI
listens to, so it can display the device).

On the Android side, each device's application is composed of several
components: -- A Recording Controller that receives start/stop
commands from the PC and controls the local recording (camera and sensor
capture). -- Separate Recorder modules for each modality: e.g.,
`CameraRecorder` for RGB video, `ThermalRecorder` for thermal imaging,
and `ShimmerRecorder` if the Android is paired to a Shimmer sensor
(see Appendix F.2 for implementation details). These recorders interface with hardware (camera APIs, etc.) and
save data to local storage. A Network Client (or Device
Connection Manager) that maintains the socket connection to the PC's
server. It listens for commands (e.g., start/stop, sync signal) and
sends back status updates or data as needed. A
FileTransferManager on Android handles
sending the recorded files to the PC upon request after recording. Utility components like a Security Manager (ensuring encryption if
TLS is used), a Storage Manager (to check available space and
organise files), etc., are also part of the design (many of these are
referenced in the architecture documentation).

Communication and Data Flow: All communication between the PC and
Android devices uses a client-server model. The PC runs the server
(listening on a specified host/port, with a maximum number of
connections defined), and each Android client connects to it when
ready. Messages are encoded in JSON and sent over a
persistent TCP socket [21]. Important message types include: device registration/hello, start
session command, stop session command, sync signal command, status
update from device, file transfer requests, etc.

During a session, the data flow is as follows: Shimmer GSR
Data: If a Shimmer sensor is directly connected to the PC, it streams
data via Bluetooth to the PC's Shimmer Manager, which then immediately
enqueues the data for writing to a CSV and also triggers any real-time
displays. If the Shimmer is connected to an Android (i.e.,
Android-mediated), the sensor data first goes to the Android (via
Bluetooth), and the Android then forwards each GSR sample (or batch of
samples) over the network to the PC. This is handled by the
`AndroidDeviceManager._on_android_shimmer_data` callback on the PC side,
which receives `ShimmerDataSample` objects from the device and processes
them similarly (see Appendix F.3 for implementation details). In both cases, each GSR sample is timestamped (using the
synchronised clock) and logged. The PC might accumulate these in memory
(e.g., in `data_queues`) briefly for processing but ultimately writes
them out via a background file-writing thread. Video and
Thermal Data: The Android devices record video and thermal streams
locally to their flash storage (to avoid saturating the network by
streaming raw video). The PC may receive low-frequency updates or
thumbnails for monitoring, but the bulk video data stays on the device
until session end. The temporal synchronisation of video with GSR is
ensured by all devices starting recording upon the same start command
and using synchronised clocks. Additionally, the PC's sync signal
(flash) provides a reference point that can be seen in the video and is
logged in the GSR timeline, tying the streams together. After the
recording, when the PC issues the file transfer, the video files are
sent to the PC. This transfer uses the network (possibly chunking files
if large). The FileTransferHandler on PC receives each chunk or file and
saves it. Because the PC knows the session start time and each video
frame's device timestamp (the Android might embed timestamp metadata in
video or provide a separate timestamp log), alignment can be done in
post-processing. There is also a possibility that the Android app sends
periodic timestamps during recording to the PC (as part of
SessionSynchroniser updates) so the PC is aware of recording progress. Time Sync and Heartbeats: Throughout a session, the
PC might send periodic time sync packets to the Androids (or the
Androids request them). The `SessionSynchronizer` on PC also keeps a
heartbeat: it tracks if it hasn't heard from a device's state in a
while, marking it offline after a threshold. Android devices
likely send a small status message every few seconds ("I'm alive,
recording, file X size = ..."). This data flow ensures the PC has
up-to-date knowledge of each device (e.g., how many frames recorded, or
storage used). Data Aggregation: Once all data reaches the PC,
the system has a session aggregation step (which can be considered
post-session). For instance, the Session Manager might invoke a function
to perform any post-processing including
post-session hand segmentation processing on the recorded video. In practice, after all files are in place, the PC could
combine or index them (for example, generating an index of timestamps).
This ensures that all data from the distributed sources is now
centralised in one place (the PC's file system) and organised.

System Architecture Diagram: *Figure 3.1 (Placeholder)* would
illustrate the above in a block diagram: a PC node on one side with
blocks for Session Manager, Shimmer Manager, Network Server, etc., and
multiple Android nodes on the other, each containing Camera, Thermal,
Shimmer (if any) and a network client. Lines would show Bluetooth links
(PC to Shimmer, or Android to Shimmer), and WiFi/LAN links between PC
and each Android. Data flows (like GSR data flowing to PC, video files
flowing after stop) would be indicated with arrows. Time sync flows (PC
broadcasting time) would also be shown. The diagram would emphasise the
star topology (PC in centre).

Key Design Considerations: The architecture ensures scalability
by decoupling data producers (devices) from the central coordinator.
Each Android operates largely independently during recording (writing to
local disk), which avoids overloading the network. The PC focuses on
low-bandwidth critical data (GSR streams, commands, and occasional
thumbnails or status). By using local storage on devices and
transferring after, the system mitigates the risk of network bandwidth
issues affecting the recording quality. The use of threads and
asynchronous I/O on the PC side (for writing files and handling multiple
sockets) ensures that adding more devices will linearly increase
resource usage but not deadlock the system.

The architecture also provides fault isolation: if one device
crashes, it does not bring down the whole system -- the PC will continue
managing others. The SessionSynchronizer component acts like a watchdog
and queue, so even if connectivity returns after a lapse, the overall
session can still be coherent.

## 3.7 Risk Management and Mitigation Strategies

The development and deployment of this multi-sensor recording system presents several technical and operational risks that require careful management:

### Technical Risks

Risk: Device Discovery Failures
- *Impact*: Android devices may not be discoverable over the network, preventing session initiation
- *Likelihood*: Medium (network configuration dependencies)
- *Mitigation*: Implemented robust Zeroconf/mDNS service with manual IP fallback; retry mechanisms with exponential backoff; clear diagnostic messages for network troubleshooting

Risk: Synchronisation Drift
- *Impact*: Temporal misalignment between devices could compromise data quality for multi-modal analysis
- *Likelihood*: Medium (clock drift over long sessions)
- *Mitigation*: Continuous NTP-style clock synchronisation during active sessions; monotonic clock sources; post-processing alignment verification; configurable tolerance thresholds

Risk: UI Responsiveness Issues
- *Impact*: Interface freezes during high data throughput could interrupt user workflow
- *Likelihood*: Low (addressed through design)
- *Mitigation*: Asynchronous background threading for all I/O operations; C++ performance-critical components; progress indicators and user feedback; graceful degradation under load

Risk: Data Transfer Integrity
- *Impact*: Corrupted or incomplete file transfers could result in data loss
- *Likelihood*: Low (robust protocol design)
- *Mitigation*: SHA-256 checksums for all file transfers; retry mechanisms with timeout handling; atomic file operations; backup storage verification

### Operational Risks

Risk: Sensor Hardware Limitations
- *Impact*: Thermal camera or GSR sensor malfunctions could affect specific data modalities
- *Likelihood*: Medium (hardware dependency)
- *Mitigation*: Modular architecture allows graceful degradation; alternative sensor support; comprehensive error reporting; device health monitoring

Risk: Network Bandwidth Constraints
- *Impact*: Insufficient network capacity could cause dropped frames or failed transfers
- *Likelihood*: Medium (environment dependent)
- *Mitigation*: Adaptive quality settings; compression algorithms; bandwidth monitoring; local storage with deferred transfer options

### Project Management Risks

Risk: Complexity Management
- *Impact*: System complexity could lead to maintenance difficulties and technical debt
- *Likelihood*: Medium (inherent multi-platform complexity)
- *Mitigation*: Implemented ADR documentation for design decisions; modular architecture with clear interfaces; comprehensive testing strategy; code quality monitoring

Risk: Integration Challenges
- *Impact*: Cross-platform compatibility issues could limit system deployment
- *Likelihood*: Low (extensive testing)
- *Mitigation*: Multi-platform testing (Windows, Linux, macOS); standardised protocols; minimal external dependencies; comprehensive documentation

The risk management approach emphasises early identification and proactive mitigation rather than reactive responses. Regular system monitoring and user feedback collection enable continuous risk assessment and mitigation strategy refinement.

------------------------------------------------------------------------

## References

See [centralised references](references.md) for all citations used throughout this thesis.
