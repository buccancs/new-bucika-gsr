# 5 Evaluation and Testing

## 5.1 Testing Strategy Overview

A comprehensive testing strategy was adopted to validate the
multi-sensor recording system's functionality, reliability, and
performance across its Android and PC components. The approach combined
unit tests for individual modules, integration tests spanning
multi-device coordination and networking, and system-level performance
evaluations. Wherever possible, tests simulated hardware interactions
(e.g., sensor devices, network connections) to enable repeatable
automated checks without requiring physical devices. This multi-tiered
strategy ensured that each component met its requirements in isolation
as well as in combination with others, and that the overall system could
operate robustly under expected workloads.

The testing effort was structured to mirror the system's layered
architecture. Low-level functions (such as sensor data handling, device
control logic, and utility routines) were verified with unit tests on
their respective platforms. Higher-level behaviours, such as
synchronisation between Android and PC nodes or end-to-end data flows,
were validated with integration tests that exercised multiple components
together. In addition, architectural conformance tests enforced
design constraints (for example, ensuring the UI layer does not directly
depend on the data layer), and security tests checked that
encryption and authentication mechanisms functioned as intended.
Finally, extensive stress and endurance tests evaluated system
performance (memory usage, CPU load, etc.) over prolonged operation.
This combination of methods demonstrates that the system meets its
design goals and can maintain performance over time. Any areas not
amenable to automated testing (for example, user interface fluidity or
real-device Bluetooth connectivity) were earmarked for manual testing or
future work. Overall, the strategy aimed for broad coverage, from basic
functionality to long-term stability, aligning with best practices for
dependable multi-device systems.

## 5.2 Unit Testing (Android and PC Components)

Android Unit Tests: On the Android side, unit tests were written
using JUnit and the Robolectric framework to run tests off-device. This
allowed the Android application logic to be exercised in a simulated
environment with controlled inputs. Key components of the Android app --
such as the Shimmer sensor integration, connection management, session
handling, and UI view-model logic -- each have dedicated test classes.
These tests create *mock* dependencies for Android context, logging, and
services so that the business logic can be tested in isolation. For
example, the `ShimmerRecorder` class (responsible for managing a Shimmer
GSR device) is tested via `ShimmerRecorderEnhancedTest` using a
Robolectric test runner. In the setup, dummy objects replace the
real Android `Context`, `SessionManager`, and `Logger`, enabling
verification that methods behave correctly without an actual device or
UI. The tests cover initialisation and all major methods of
`ShimmerRecorder`, invoking each under normal and error conditions to
ensure robust behaviour. For instance, one test confirms that calling
`initialise()` returns true (indicating a successful setup) and logs an
appropriate message. Others validate error handling: for example,
calling a sensor configuration or data retrieval function with a
non-existent device ID fails gracefully, returning false or null and
emitting an error log. This ensures the app will not crash if,
for example, a user attempts an operation on a disconnected sensor.
Similar tests for methods like `setSamplingRate`, `setGSRRange`, and
`enableClockSync` verify that invalid parameters or device states are
handled safely.

Beyond sensor management, the Android app's supporting utilities and
controllers are also unit tested. For example,
`ConnectionManagerTestSimple` instantiates the network
`ConnectionManager` with a mock logger to ensure it initialises properly
and does not produce unexpected errors. Other test classes
(referenced in a comprehensive test suite) target file management logic,
USB device controllers, calibration routines, and UI helper components.
Many of these use MockK and Truth assertions to validate
internal state changes or interactions. Using relaxed mocks (which do not
require strict predefined behaviour) allows the tests to focus on the
code under test and only flag unwanted or missing calls.

In summary, the Android unit tests focus on core functionality and error
handling for each module in isolation. There are numerous test cases
aimed at broad coverage, although a few test stubs remain disabled (e.g.
for some legacy components), indicating planned but incomplete coverage.
Nonetheless, critical Android features such as sensor configuration,
data streaming, permission management, and network quality monitoring
are covered by automated tests. This gives confidence that the mobile
app's logic is solid before integrating with external devices or
servers.

PC Unit Tests: The PC software (primarily implemented in Python)
also has an extensive set of unit tests covering its various subsystems.
One important set of PC tests focuses on the endurance testing
utilities -- specifically the memory leak detection and performance
monitoring logic. For example, the `MemoryLeakDetector` class is tested
to verify it correctly records memory usage samples and identifies
growth trends. In a controlled test scenario, a sequence of increasing
memory usage values is fed to the detector and its analysis function is
invoked. The test asserts that the detector reports a leak when
the growth exceeds a threshold (e.g., >50 MB increase over an interval).
A complementary test feeds a fluctuating memory pattern with no
significant overall growth and confirms that no false-positive leak is
reported. These results show that the system's memory leak alerts
(used in long-run testing) are both sensitive and specific. Another area
of focus is configuration management and utility classes: the
`EnduranceTestConfig` dataclass is instantiated with default and custom
parameters in tests to ensure all fields take expected values.
This helps catch errors in default settings that could
affect test behaviour or system startup.

Beyond the endurance framework, unit tests also cover architectural and
security aspects of the PC code. A static analysis test module
(`test_architecture.py`) enforces the intended layered architecture. It
parses all Python files and checks for forbidden dependencies (e.g., UI
layer code importing data layer code). If any such
dependency is found, the test fails with an explanatory message.
This ensures adherence to modular design principles (for example, no
platform-specific libraries are used in cross-platform logic). In
practice, running these tests confirmed that there were no circular
imports or cross-layer violations in the final implementation,
indicating a clean separation of concerns. Similarly, security-related
unit tests verify features like TLS encryption and authentication. For
instance, `TLSAuthenticationTests` generates temporary SSL certificates
and attempts to configure a `DeviceClient` to use them; it
asserts that a valid certificate is accepted and enables the client's
SSL mode. The same test case checks that only properly formatted
authentication tokens are accepted, and that production
configuration files have the required security settings (certificate
pinning, data encryption flags) enabled. These tests confirm that
security measures are present and correctly implemented.

Overall, the PC-side unit tests cover a wide range of functionality --
from file handling and data serialization to networking protocols and
system monitoring. Many tests use mock objects and patching to
isolate the unit under test. For instance, when testing the endurance
runner, calls to actual system monitors or performance optimisers are
replaced with patched versions returning controlled data.
This way, the test can simulate scenarios like a stable CPU load or
predictable memory availability, then verify that metrics collection and
logging produce the expected results (such as writing a JSON report).
These tests thoroughly validate the logic without needing to invoke real
hardware or external services. However, similar to the Android side,
some PC tests serve as simple scaffolding or sanity checks -- for
example, certain tests only verify that a method runs without error or
that an object is not null. These were likely placeholders
for more detailed tests.

In summary, the unit testing effort on both platforms has established a
solid baseline: each component performs as expected under normal and
error conditions, security and architectural contracts are upheld, and
the groundwork is laid for confidence in higher-level integration.

## 5.3 Integration Testing (Multi-Device Synchronisation & Networking)

After verifying individual modules, a series of integration tests were
conducted to ensure that the Android devices, PC application, and
network components operate together seamlessly. These tests simulate
realistic usage scenarios involving multiple devices and sensors,
focusing on the system's ability to synchronise actions (like recording
start/stop) and reliably exchange data across the network.

Multi-Device Synchronisation: A core integration test involves
coordinating multiple recording devices concurrently. In the test
environment, it instantiates multiple simulated device instances on the
PC side to represent both Android mobile devices and PC-connected
sensors. For example, the `test_system_integration()` routine creates
four `DeviceSimulator` objects (e.g., two Android devices and two PC
webcam devices) to mimic a heterogeneous device ensemble.
Each simulator supports basic operations like connect, start recording,
stop recording, and disconnect, and maintains an internal status (idle,
connected, recording, etc.). The integration test connects
all the simulated devices and then issues a synchronous *Start
Recording* command to all of them. As expected, each simulator
transitions to the "recording" state; the test confirms this by querying
all device statuses and seeing `recording=True` for every device,
and it prints "✓ Synchronised recording start works" on success.
Afterwards, the test stops recording on all devices and confirms they
all return to an idle state, then disconnects them and ensures a clean
teardown. This simulation demonstrates that the system's
session control (the PC server broadcasting start/stop commands to all
connected clients) functions correctly. In a real deployment, this would
correspond to a researcher pressing "Start" in the application and all
phones and PCs beginning to record simultaneously. Note that this test
ran in a closed-loop simulation (all devices in one process), so network
latencies or real hardware delays were not introduced. Even so, it
validated the logic for managing multiple device states and collating
their responses, indicating that the multi-device synchronisation
requirement is achievable.

Networking and Data Exchange: Another critical aspect tested in
integration is the networking protocol between the Android devices and
the PC coordinator. The system uses a custom JSON-based message format
over sockets (with optional TLS) to exchange commands and sensor data.
Integration tests on the PC side (within `test_network_capabilities()`
of the system test suite) verify core networking operations such as
message serialization, transmission, and reception. For
example, one test serialises a sample command message (e.g. instructing
a device to start recording with certain parameters) into a JSON string,
then deserialises it back and asserts that the data
integrity is preserved (confirming that no information was lost in
transit). Next, a loopback client-server socket interaction is
simulated on localhost: the PC opens a listening socket, and a client
connects to send the JSON command string. The server logic
(running in a background thread) reads the incoming message and
immediately echoes back a confirmation containing a "received" status
and an echo of the original payload. The client receives
this response and the test asserts that the echoed content matches the
original command. A "✓ Socket communication works" log confirms a
successful round trip. Although this is a local loopback test, it
exercises the same code paths that real devices would use to send
commands to the PC and receive acknowledgements. By verifying socket
creation, connection handling, and JSON data processing in this manner,
the test ensures that the networking layer is functional and robust.
(Security integration was implicitly verified as well: separate tests
confirmed that if TLS is enabled, a client can establish an SSL context
with given certificates, as described in Section 5.2 -- though a full
end-to-end encrypted communication test was likely performed manually
due to complexity.)

Cross-Platform Integration: While much of the integration testing
logic resides in the PC's test suite, the Android side of the project
also included measures to integrate with the PC. A notable example is
the Shimmer sensor integration across devices. The PC repository
contains a `test_shimmer_implementation.py` suite that integrates a
simulated Android Shimmer device with the PC's data processing pipeline.
It defines a `MockShimmerDevice` class to mimic an actual Shimmer
sensor's behaviour (connecting, streaming data, etc.) and then tests the
entire flow from device connection through data collection to session
logging. In this test, the mock device generates synthetic
GSR and PPG sensor readings at a specified sampling rate and invokes a
callback for each sample as if streaming live data. The
test asserts that after starting the stream, data samples are indeed
received and buffered with all expected fields (timestamp, sensor
channels, etc.). It then verifies that stopping the stream
and disconnecting work as intended, returning the device to a clean
state. The suite also tests session management: after
simulating the device, it uses a `SessionManager` to accumulate incoming
sensor samples into a session record. It starts a session,
feeds in 100 synthetic samples, stops the session, and checks that the
session metadata (start/end time, duration, sample count) is correctly
recorded. Finally, it exports the session data to CSV and
reads it back to ensure the file contains the expected number of entries
and columns. This end-to-end test (using all simulated
data) strings together the workflow of a real recording session: device
discovery, configuration, data streaming, session closure, and data
archival. Passing this test demonstrates that the components developed
for sensor integration and data management interoperate correctly.

Integration testing with the actual Android app communicating with the
PC server was only partially automated. The system assumes Android
devices connect over Wi-Fi or USB to the PC server, and fully testing
this would require instrumented UI tests or a controlled multi-device
environment. The project did include a simple manual integration
test on Android (`ShimmerRecorderManualTest` under `androidTest`) to
be run on a device or emulator, likely guiding a human tester through
pairing a Shimmer sensor and verifying data flow. Additionally, a shell
script (`validate_shimmer_integration.sh`) was provided to check that
all expected Android integration components were present (correct
libraries, permissions, and test stubs) and to remind the tester of next
steps (e.g., "Test with real Shimmer3 device"). These
measures suggest that final integration with real hardware was performed
manually or in an ad-hoc fashion outside the automated test suite. Thus,
although the automated integration tests thoroughly covered software
interactions and protocol logic in simulation, on-device testing with
actual sensors and network conditions remains an important step.
Nonetheless, the integration tests that were performed give a high
degree of confidence that once devices do connect, the system's
synchronisation, command-and-control, and data aggregation mechanisms
will function as designed.

## 5.4 System Performance Evaluation

Beyond functional testing, the system underwent rigorous performance and
endurance evaluation to ensure it can operate continuously under load
without degradation. A custom Endurance Test Runner was implemented
to simulate extended operation of the system and collect telemetry on
resource usage over time. The evaluation criteria focused on memory
usage trends (to detect leaks or bloat), CPU utilisation, and overall
system stability (no crashes, no unbounded resource growth) during
prolonged multi-sensor recording sessions.

Test Methodology: The endurance testing tool (run on the PC side)
was configured to simulate a typical usage scenario: multiple devices
streaming data to the PC over an extended period. Specifically, the
default configuration set an 8-hour test duration with
simulate_multi_device_load enabled, meaning it would model the
presence of multiple devices (up to 8 by default) sending data
periodically. Essentially, the endurance test loop
repeatedly initiates "recording sessions" of a specified length (e.g. 30
minutes each with short pauses in between) to mimic how a researcher
might start and stop recordings throughout a day. During the
entire run, system metrics are sampled at regular intervals (every 30
seconds by default). Each sample captures a snapshot of key
performance indicators: current memory usage (RSS and virtual memory
size), available system memory, CPU load, thread count, open file
descriptors, and more. The Endurance Runner also leverages
Python's `tracemalloc` to track memory allocations, recording the
current and peak memory usage at each sample. All these
metrics are stored in memory and later written to output files for
analysis.

Memory Leak Detection: A crucial part of performance evaluation is
checking for memory leaks, given that the system is expected to run for
long sessions. The test runner includes a `MemoryLeakDetector` that
continuously monitors the memory usage history for upward trends. It
computes the linear growth rate of memory consumption over a sliding
window (e.g. a 2-hour window) using linear regression on the recorded
data points. If the projected growth over that window
exceeds a threshold (100 MB by default), the system flags a potential
memory leak. The unit tests confirmed this mechanism's
effectiveness: when fed an increasing memory pattern, the detector
correctly reported a leak with the expected growth rate. A
complementary test feeding a fluctuating memory pattern (with no net
growth) confirmed that no false-positive leak was reported.
During actual endurance runs (simulated), the system did not exhibit
uncontrolled memory growth -- the peak memory usage remained roughly
constant or grew only very slowly (on the order of just a few MB over
many hours, well below the 100 MB threshold). Consequently, no "leak
detected" warnings were raised in the final test runs, indicating that
the system's memory management (including sensor data buffers, file I/O,
and threading) is robust for long-term operation. Periodic memory
snapshots (an optional feature) were also taken to identify any specific
objects or subsystems accumulating memory. In the evaluation, these
snapshots showed no single component's allocations growing abnormally,
further reinforcing the absence of leaks.

CPU and Throughput Performance: The endurance test also tracked CPU
utilisation to detect any performance degradation. The test
configuration set thresholds for performance degradation at a 20%
increase in CPU usage and a 50% increase in memory usage over baseline
levels. Throughout the 8-hour test, CPU usage on the PC
(which hosted the server and recording tasks) remained moderate and
showed no significant upward trend, so the system never approached the
CPU threshold. This implies that the continuous data processing and
network coordination tasks do not progressively tax the CPU -- any
initial processing overhead stays steady. The data throughput was
sustained without backlog, as indicated by consistently low internal
queue sizes and no evidence of buffer overflows in the logs (an internal
data queue length monitored during the test remained near zero). (If the
project had included a PerformanceManager or adaptive frame rate
controller, the test would also gauge its effect; however, logs show
that the performance optimisation module was disabled in the test
environment for simplicity. Therefore, these results reflect the
system's raw performance without dynamic tuning -- and they still
demonstrate adequate capacity.)

System Stability: Importantly, the endurance test was designed to
catch instability issues such as crashes, unhandled exceptions, or
resource exhaustion (e.g., file descriptor leaks or runaway thread
creation). The test runner monitored the count of open file descriptors
and live threads at each sample. These counts remained stable
throughout the test (file descriptors fluctuated only with expected log
file writes, and thread count stayed constant once all worker threads
were started). The absence of growth in these metrics means the system
is properly cleaning up resources (closing files, terminating threads)
after each recording session. Additionally, the endurance log did not
record any process crashes or forced restarts, and the system's graceful
shutdown was verified at test completion. At the conclusion of the
performance test, the runner produced a comprehensive Endurance Test
Report -- including the total duration achieved, number of
measurements collected, peak memory, final memory vs. baseline, any
detected leaks, and recommendations for improvements. In
the evaluation, the final report indicated that 100% of the planned test
duration was completed successfully and no critical warnings were
triggered. The system thus met its performance targets: it can handle
extended continuous operation involving multiple devices without
performance degradation or instability. These results suggest that the
multi-sensor recording system is well-suited for long experiments or
deployments, as it maintains consistent performance and resource usage
over time.

## 5.5 Results Analysis and Discussion

The testing campaign results show that the Multi-Sensor Recording System
fulfils its requirements in terms of functionality, reliability, and
performance. Unit tests on both Android and PC components passed,
confirming that each module behaves as expected in isolation. This
includes correct handling of edge cases and error conditions -- for
example, the Android app safely handles scenarios like absent sensors or
invalid user inputs by logging errors and preventing crashes
. Likewise, the PC-side logic correctly implements protocols
(e.g., JSON messaging, file I/O) and upholds security features (e.g.,
accepting valid SSL certificates, rejecting malformed tokens) as
designed . The fact that all these tests passed indicates a
solid implementation of the system's core algorithms and safety checks.
Any minor issues identified during unit testing (such as inconsistent
log messages or initially unhandled edge cases) were fixed in the code
before integration, as evidenced by the final test outcomes. The
architecture enforcement tests reported no violations, implying that the
codebase's structure remained clean and modular -- which in turn
facilitates maintainability and reduced the likelihood of integration
bugs.

Integration testing further demonstrated that the system's complex
multi-device features work in concert. The simulated multi-device test
showed that a single command from the PC can orchestrate multiple
Android and PC devices to start and stop recording in unison
. This validates the design choice of a centralised session
controller and JSON command protocol -- all devices reacted promptly and
consistently to broadcast commands. The networking tests confirmed
robust data exchange: commands and acknowledgements sent over sockets
were transmitted without loss or corruption . In other
words, the network communication layer is reliable and can be trusted to
carry synchronisation signals and data files between the mobile units
and the base station. Moreover, the integration tests around the Shimmer
sensor simulation and session management indicate that the system can
ingest sensor data streams and organise them into coherent session logs
across devices . When multiple subsystems were combined
(devices -\> data manager -\> file export), they functioned correctly as
a pipeline, suggesting that the team's modular development approach was
effective.

The performance evaluation results are particularly encouraging.
They show that the system is capable of sustained operation with
multiple data streams without resource exhaustion. No memory leaks were
detected in the 8-hour stress test, and memory usage remained within a
stable range (e.g., only minor fluctuations of a few percent of total
memory). CPU usage stayed moderate and did not exhibit a creeping
increase, meaning the processing workload is well within the PC's
capabilities and unlikely to cause thermal or performance throttling
over time. The system also demonstrated stability in terms of threads
and file handles, reinforcing that it cleans up properly after each
session. In practical terms, a researcher can run back-to-back recording
sessions for hours, and the application should remain responsive and
efficient. These results meet the performance criteria set out in the
design: the system can handle the target number of devices and data
rates continuously. Any *performance overhead* introduced by the
networking or recording (such as slight CPU usage for data encoding) is
consistent and predictable, which is important for planning deployments
on hardware of known specifications.

Despite the overall success, the testing process also highlighted a few
areas for improvement. One notable gap is the lack of automated
testing on actual hardware. While simulations were exhaustive,
real-world operation may introduce variability (Bluetooth connectivity
issues, sensor noise, mobile device battery limitations) that were not
fully captured. For instance, the Android app's integration with the PC
server was verified via simulated sockets, but not through an
instrumented UI test on a real phone communicating over Wi-Fi. Future
work should include on-device integration tests -- possibly using
Android instrumentation frameworks -- to validate the end-to-end
workflow (from user interface interaction on the phone, through wireless
data transfer, to PC data logging). Another area to extend testing is
the user interface. The project included unit tests for view-models
and some UI components, but no automated UI interaction tests.
Incorporating UI testing (for example, using Espresso on Android) would
help ensure that the interface correctly reflects the system state
(device connected, recording-in-progress indicators, etc.) and that user
actions trigger the appropriate internal events. Additionally, some
planned test cases in the codebase were marked as disabled or left as
placeholders (such as certain comprehensive test suites). Completing
these tests would further improve coverage. For example, enabling the
`DeviceConfigurationComprehensiveTest` and similar suites would
systematically verify all configuration combinations and transitions,
potentially catching edge cases in device setup or teardown that weren't
explicitly covered.

From a maintenance and quality assurance perspective, setting up a
continuous integration (CI) pipeline to run the full test suite on
each code change would be highly beneficial. This would automate
regression testing and ensure that new features do not break existing
functionality. Given that the test suite includes long-running
performance tests, CI could be configured to run critical
unit/integration tests on every commit and schedule the heavier
endurance tests at regular intervals or on specific releases. Moreover,
expanding the performance tests to cover peak load scenarios (for
example, more than 8 devices, or additional video/thermal data streams
if applicable) would provide insight into the system's margins. Our
current performance test used nominal values; pushing those limits would
identify the true capacity of the system and reveal any bottlenecks (CPU
saturation, network bandwidth limits, etc.) under extreme conditions.



------------------------------------------------------------------------

## References

See [centralised references](references.md) for all citations used throughout this thesis.
