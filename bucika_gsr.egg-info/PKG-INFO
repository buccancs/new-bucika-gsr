Metadata-Version: 2.1
Name: bucika-gsr
Version: 0.1.0
Summary: Comprehensive evaluation suite for machine learning metrics
Author: buccancs
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Provides-Extra: dev
License-File: LICENSE

# Bucika GSR - Comprehensive Evaluation Suite

A comprehensive evaluation suite for machine learning metrics supporting classification, regression, clustering, and ranking tasks.

## Features

- **Classification Metrics**: Accuracy, Precision, Recall, F1-Score, ROC-AUC
- **Regression Metrics**: MAE, MSE, RMSE, RÂ², MAPE, Mean Error
- **Clustering Metrics**: Silhouette Score, Adjusted Rand Index, Adjusted Mutual Information, Calinski-Harabasz Score, Davies-Bouldin Score  
- **Ranking Metrics**: NDCG, MAP, Precision@K, Recall@K
- **Extensible Framework**: Easy to add custom metrics
- **Command-line Interface**: Simple CLI for quick evaluations
- **Comprehensive Testing**: Full test coverage for all metrics

## Installation

```bash
# Install in development mode
pip install -e .

# Install with development dependencies
pip install -e ".[dev]"
```

## Quick Start

### Using the Python API

```python
import numpy as np
from bucika_gsr import EvaluationSuite
from bucika_gsr.metrics import classification

# Create evaluation suite
suite = EvaluationSuite()

# Add metrics
suite.add_metric(classification.Accuracy())
suite.add_metric(classification.F1Score())

# Evaluate
y_true = np.array([0, 1, 1, 0, 1])
y_pred = np.array([0, 1, 0, 0, 1])

results = suite.evaluate(y_true, y_pred)
print(results)  # {'accuracy': 0.6, 'f1_binary': 0.6666666666666666}
```

### Using the CLI

```bash
# Create sample data file
echo '{"y_true": [0, 1, 1, 0, 1], "y_pred": [0, 1, 0, 0, 1]}' > data.json

# Evaluate classification metrics
bucika-eval --type classification --data data.json

# List available metrics
bucika-eval --list --type classification

# Evaluate specific metrics only  
bucika-eval --type classification --data data.json --metrics accuracy f1_binary
```

## Supported Metrics

### Classification
- `accuracy`: Overall accuracy
- `precision_*`: Precision with different averaging (binary, macro, micro)
- `recall_*`: Recall with different averaging (binary, macro, micro)  
- `f1_*`: F1-score with different averaging (binary, macro, micro)
- `roc_auc_*`: ROC AUC score

### Regression
- `mae`: Mean Absolute Error
- `mse`: Mean Squared Error  
- `rmse`: Root Mean Squared Error
- `r2`: R-squared (coefficient of determination)
- `mape`: Mean Absolute Percentage Error
- `me`: Mean Error (bias)

### Clustering
- `silhouette_score`: Silhouette Score (requires feature matrix X)
- `adjusted_rand_score`: Adjusted Rand Index
- `adjusted_mutual_info_score`: Adjusted Mutual Information Score  
- `calinski_harabasz_score`: Calinski-Harabasz Score (requires feature matrix X)
- `davies_bouldin_score`: Davies-Bouldin Score (requires feature matrix X)

### Ranking
- `ndcg`: Normalized Discounted Cumulative Gain
- `ndcg@k`: NDCG at cutoff k (k=5,10)
- `map`: Mean Average Precision
- `precision@k`: Precision at cutoff k (k=1,5,10)
- `recall@k`: Recall at cutoff k (k=5,10)

## Advanced Usage

### Adding Custom Metrics

```python
from bucika_gsr.core import BaseMetric
import numpy as np

class CustomAccuracy(BaseMetric):
    def __init__(self):
        super().__init__("custom_accuracy", higher_is_better=True)
    
    def compute(self, y_true, y_pred, **kwargs):
        return float(np.mean(y_true == y_pred))

# Use custom metric
suite = EvaluationSuite()
suite.add_metric(CustomAccuracy())
```

### Clustering with Feature Matrix

```python
from bucika_gsr.metrics import clustering

# For clustering metrics that require features
X = np.random.rand(100, 2)  # Feature matrix
y_true = np.array([0]*50 + [1]*50)  # True clusters
y_pred = np.array([0]*45 + [1]*5 + [1]*45 + [0]*5)  # Predicted clusters

suite = EvaluationSuite()
suite.add_metric(clustering.SilhouetteScore())

results = suite.evaluate(y_true, y_pred, X=X)
```

## Testing

```bash
# Run all tests
pytest

# Run tests with coverage
pytest --cov=bucika_gsr

# Run specific test file
pytest tests/test_suite.py
```

## Development

```bash
# Install development dependencies
pip install -e ".[dev]"

# Format code
black bucika_gsr/ tests/

# Lint code  
flake8 bucika_gsr/ tests/
```

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.
